<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[notes]]></title><description><![CDATA[Obsidian digital garden]]></description><link>http://github.com/dylang/node-rss</link><image><url>lib/media/favicon.png</url><title>notes</title><link/></image><generator>Webpage HTML Export plugin for Obsidian</generator><lastBuildDate>Sun, 23 Mar 2025 08:22:05 GMT</lastBuildDate><atom:link href="lib/rss.xml" rel="self" type="application/rss+xml"/><pubDate>Sun, 23 Mar 2025 08:22:05 GMT</pubDate><ttl>60</ttl><dc:creator/><item><title><![CDATA[Adapters in 2D and 3D Vision Foundation Models]]></title><description><![CDATA[ 
 <br><br><br>Foundation models are large, pre-trained models designed to capture general representations from vast datasets, which can then be adapted to specific downstream tasks. In 2D vision, examples include Vision Transformers (ViTs) like CLIP or DINO, while in 3D vision, models like PointNet handle point clouds for tasks such as shape recognition. Fully retraining these models for new tasks is computationally expensive due to their size and complexity. Adapters offer an efficient solution by introducing a small number of trainable parameters to adapt the model, while keeping most of the pre-trained parameters frozen.<br>This document covers:<br>
<br>Basics of LoRA (Low-Rank Adaptation)
<br>Other Adapter Types
<br>Implementation Details
<br>Pros and Cons of Adapters
<br>Possible Optimizations
<br><br><br><br>LoRA, or Low-Rank Adaptation, is a technique that adapts pre-trained models by adding low-rank updates to specific weight matrices, typically in the attention layers of transformer-based models. Instead of updating the entire weight matrix  during fine-tuning, LoRA approximates the weight update  as the product of two low-rank matrices  and , where . Here,  has shape  and  has shape , with  (the rank) being much smaller than , the original dimension of . During training, only  and  are updated, while  remains frozen.<br>In vision transformers, LoRA is commonly applied to the query () and value () projection matrices in the self-attention mechanism:<br>
<br>Query: 
<br>Value: 
<br>This approach reduces the number of trainable parameters significantly. For a  matrix, full fine-tuning requires  parameters, whereas LoRA requires only , where .<br><br>LoRA is ideal for adapting large foundation models efficiently, making it applicable to both 2D vision (e.g., ViTs) and 3D vision (e.g., MLPs in PointNet), where attention or linear layers are prevalent.<br>
<br>Research Insight: The paper <a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2106.09685" rel="noopener nofollow" class="external-link" href="https://arxiv.org/abs/2106.09685" target="_blank">"LoRA: Low-Rank Adaptation of Large Language Models"</a> demonstrates that LoRA can reduce trainable parameters by up to 10,000 times compared to full fine-tuning, with comparable or better performance.
<br>Implementation Example: For vision models, repositories like <a data-tooltip-position="top" aria-label="https://github.com/JamesQFreeman/LoRA-ViT" rel="noopener nofollow" class="external-link" href="https://github.com/JamesQFreeman/LoRA-ViT" target="_blank">"LoRA-ViT"</a> show its application, achieving 1.8x to 1.9x faster training on M1 Pro hardware.
<br><br><br>Beyond LoRA, several adapter types are used to adapt 2D and 3D vision foundation models, offering flexibility based on task and architecture.<br><br>Adapter modules are small neural networks inserted between the layers of a pre-trained model. In transformers, they are typically placed after the attention or feed-forward layers. A common design includes:<br>
<br>A down-projection (reducing dimension from  to a smaller ),
<br>A non-linearity (e.g., ReLU),
<br>An up-projection (returning to dimension ).
<br>The output is combined with the input via a residual connection:<br>
<br>
<br>2D Vision Example: The <a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2205.08534" rel="noopener nofollow" class="external-link" href="https://arxiv.org/abs/2205.08534" target="_blank">"ViT-Adapter"</a> paper proposes a composition of three modules—spatial prior module, spatial feature injector, and multi-scale feature extractor—enhancing ViT for dense prediction tasks like segmentation and detection. The <a data-tooltip-position="top" aria-label="https://github.com/czczup/ViT-Adapter" rel="noopener nofollow" class="external-link" href="https://github.com/czczup/ViT-Adapter" target="_blank">GitHub repository</a> provides implementations showing competitive performance.
<br>3D Vision Example: Adapter modules can be added after MLPs in models like PointNet or operate on vertex/edge features in mesh processing models like MeshCNN.
<br><br>Prompt tuning involves adding learnable tokens or "prompts" to the input sequence of the model. In 2D vision transformers, this might mean appending extra patch embeddings to the sequence of image patches.<br>
<br>2D Vision Example: The <a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2203.12119" rel="noopener nofollow" class="external-link" href="https://arxiv.org/abs/2203.12119" target="_blank">"Visual Prompt Tuning"</a> paper introduces VPT, adding less than 1% of model parameters in the input space while freezing the backbone, achieving significant performance gains over full fine-tuning, especially in low-data regimes.
<br>3D Vision Example: Prompts can be added as extra point features in point cloud inputs, as seen in the <a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2310.03059" rel="noopener nofollow" class="external-link" href="https://arxiv.org/abs/2310.03059" target="_blank">"Point-PEFT"</a> framework.
<br><br>For 3D vision models, specialized adapters handle point clouds or meshes.<br>
<br>Point-PEFT: The <a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2310.03059" rel="noopener nofollow" class="external-link" href="https://arxiv.org/abs/2310.03059" target="_blank">"Point-PEFT: Parameter-Efficient Fine-Tuning for 3D Pre-trained Models"</a> paper introduces a PEFT method for point clouds, including:

<br>Point-prior Prompt: Learnable prompt tokens enhanced by a memory bank with domain-specific knowledge.
<br>Geometry-aware Adapter: Inserted after self-attention and FFN, aggregating local geometric information via FPS, k-NN, pooling, and propagation.

  This framework achieves high accuracy on benchmarks like ModelNet40 (94.2% with 0.8M parameters) and ScanObjectNN (89.1% with 0.7M parameters), outperforming other PEFT methods with fewer parameters.<br>

<br><br><br>Implementing adapters requires slight modifications to the foundation model's architecture. Below are examples for each type, with a focus on 2D and 3D vision contexts.<br><br>In a 2D Vision Transformer, LoRA modifies the linear layers in the attention mechanism. Here's a simplified PyTorch example:<br>import torch
import torch.nn as nn

class LoRALayer(nn.Module):
    def __init__(self, original_layer, rank):
        super().__init__()
        self.original_layer = original_layer
        self.A = nn.Parameter(torch.randn(original_layer.in_features, rank))
        self.B = nn.Parameter(torch.randn(rank, original_layer.out_features))
    
    def forward(self, x):
        original_output = self.original_layer(x)
        lora_update = x @ self.A @ self.B
        return original_output + lora_update

# Usage in a ViT
original_layer = nn.Linear(768, 768)  # Example dimension from ViT
lora_layer = LoRALayer(original_layer, rank=8)
<br>
<br>3D Vision Note: For models like PointNet, similar modifications can be applied to linear layers.
<br>Practical Guide: The <a data-tooltip-position="top" aria-label="https://huggingface.co/docs/peft/main/en/task_guides/image_classification_lora" rel="noopener nofollow" class="external-link" href="https://huggingface.co/docs/peft/main/en/task_guides/image_classification_lora" target="_blank">Hugging Face PEFT documentation</a> provides a guide for LoRA in image classification using ViT, with settings like r=16, lora_alpha=16, and target modules ["query", "value"], reducing trainable parameters to 0.77% of the original model.
<br><br>For adapter modules inserted after a transformer layer:<br>class Adapter(nn.Module):
    def __init__(self, d, d_adapter):
        super().__init__()
        self.down = nn.Linear(d, d_adapter)
        self.relu = nn.ReLU()
        self.up = nn.Linear(d_adapter, d)
    
    def forward(self, x):
        return x + self.up(self.relu(self.down(x)))

# Usage
adapter = Adapter(d=768, d_adapter=64)  # Bottleneck size of 64
<br>
<br>3D Vision Note: This can be placed after PointNet's per-point MLPs.
<br>Implementation Example: The <a data-tooltip-position="top" aria-label="https://github.com/czczup/ViT-Adapter" rel="noopener nofollow" class="external-link" href="https://github.com/czczup/ViT-Adapter" target="_blank">ViT-Adapter GitHub repository</a> includes Colab notebooks for detection and segmentation.
<br><br>For prompt tuning in a 2D Vision Transformer:<br>class PromptedViT(nn.Module):
    def __init__(self, vit_model, num_prompts, d):
        super().__init__()
        self.vit_model = vit_model
        self.prompts = nn.Parameter(torch.randn(num_prompts, d))
    
    def forward(self, x):
        batch_size = x.size(0)
        prompts = self.prompts.unsqueeze(0).repeat(batch_size, 1, 1)
        x = torch.cat([prompts, x], dim=1)  # Prepend prompts to patch embeddings
        return self.vit_model(x)

# Usage
prompted_model = PromptedViT(vit_model, num_prompts=10, d=768)
<br>
<br>3D Vision Note: In 3D, prompts can be added as extra point features, as in Point-PEFT, where the optimal prompt length is 10.
<br><br>Point-PEFT's implementation includes:<br>
<br>Point-prior Prompt MLP inner dimension: 8.<br>

<br>Geometry-aware Adapter first MLP inner dimension: 16; last MLP dimensions: (8,16,16) for ScanObjectNN, (16,16,16) for ModelNet40.<br>

<br>k-NN settings: N,k as (32,16) for Point-BERT, (32,8) for Point-M2AE, (64,16) for Point-MAE.<br>

<br>Code Availability: <a data-tooltip-position="top" aria-label="https://github.com/Ivan-Tang-3D/Point-PEFT" rel="noopener nofollow" class="external-link" href="https://github.com/Ivan-Tang-3D/Point-PEFT" target="_blank">Point-PEFT GitHub Repository</a><br>

<br><br><br>Adapters offer significant benefits but also come with trade-offs. Here's a detailed breakdown:<br><br>
<br>Research Insight: Point-PEFT achieves 94.2% accuracy on ModelNet40 with 0.8M parameters versus 93.2% with 22.1M for full fine-tuning, highlighting adapters' potential to outperform full fine-tuning in some cases.
<br><br><br>To enhance the effectiveness and efficiency of adapters, consider the following optimizations:<br>
<br>LoRA Optimizations:

<br>Rank Selection: Choose an optimal rank  based on task complexity; for example, the Hugging Face guide uses r=16 for ViT.
<br>Efficient Computation: Use optimized matrix multiplication libraries to speed up .


<br>Adapter Module Optimizations:

<br>Bottleneck Size: Minimize  to reduce parameters while retaining expressiveness.
<br>Layer Sharing: Share adapter weights across multiple layers to further cut parameter counts.


<br>Prompt-based Optimizations:

<br>Initialization: Initialize prompts using statistics from pre-training data to improve convergence.
<br>Prompt Count: Experiment with the number of prompts; Point-PEFT finds length 10 optimal.


<br>General Optimizations:

<br>Mixed Precision Training: Use 16-bit precision to reduce memory usage during adapter training.
<br>Gradient Checkpointing: Trade computation for memory by recomputing intermediate activations.
<br>Sparse Structures: In 3D vision, leverage sparsity (e.g., in point clouds) to design adapters focusing on local neighborhoods.


<br>3D-Specific Optimizations:

<br>Exploit spatial structure in point clouds or meshes by designing adapters that operate on local feature groups, enhancing geometric awareness.


<br><br><br>This document provides a comprehensive overview of adapters in 2D and 3D vision foundation models. Starting with LoRA, we explored its low-rank approach, then covered adapter modules, prompt-based methods, and 3D-specific adapters like Point-PEFT. Implementation details, pros, cons, and optimizations offer a holistic view, enabling efficient adaptation of large models to new tasks with minimal computational overhead.<br><br>
<br><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2106.09685" rel="noopener nofollow" class="external-link" href="https://arxiv.org/abs/2106.09685" target="_blank">LoRA: Low-Rank Adaptation of Large Language Models</a>
<br><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2203.12119" rel="noopener nofollow" class="external-link" href="https://arxiv.org/abs/2203.12119" target="_blank">Visual Prompt Tuning</a>
<br><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2310.03059" rel="noopener nofollow" class="external-link" href="https://arxiv.org/abs/2310.03059" target="_blank">Point-PEFT: Parameter-Efficient Fine-Tuning for 3D Pre-trained Models</a>
<br><a data-tooltip-position="top" aria-label="https://huggingface.co/docs/peft/main/en/task_guides/image_classification_lora" rel="noopener nofollow" class="external-link" href="https://huggingface.co/docs/peft/main/en/task_guides/image_classification_lora" target="_blank">Image classification using LoRA</a>
<br><a data-tooltip-position="top" aria-label="https://github.com/czczup/ViT-Adapter" rel="noopener nofollow" class="external-link" href="https://github.com/czczup/ViT-Adapter" target="_blank">Vision Transformer Adapter for Dense Predictions</a>
<br><a data-tooltip-position="top" aria-label="https://github.com/KMnP/vpt" rel="noopener nofollow" class="external-link" href="https://github.com/KMnP/vpt" target="_blank">Visual Prompt Tuning [ECCV 2022]</a>
<br><a data-tooltip-position="top" aria-label="https://github.com/JamesQFreeman/LoRA-ViT" rel="noopener nofollow" class="external-link" href="https://github.com/JamesQFreeman/LoRA-ViT" target="_blank">Low rank adaptation for Vision Transformer</a>
<br><a data-tooltip-position="top" aria-label="https://github.com/Ivan-Tang-3D/Point-PEFT" rel="noopener nofollow" class="external-link" href="https://github.com/Ivan-Tang-3D/Point-PEFT" target="_blank">Point-PEFT</a>
<br><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2205.08534" rel="noopener nofollow" class="external-link" href="https://arxiv.org/abs/2205.08534" target="_blank">ViT-Adapter Paper</a>
]]></description><link>vision_adapters.html</link><guid isPermaLink="false">vision_adapters.md</guid><pubDate>Sun, 23 Mar 2025 08:21:54 GMT</pubDate></item></channel></rss>